from langchain_community.vectorstores import FAISS
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.llms import Ollama
import warnings
warnings.filterwarnings("ignore", category=DeprecationWarning)


DB_PATH = "vectorstore"

# 1Ô∏è‚É£ Carrega o banco vetorial
def load_vectorstore():
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )
    return FAISS.load_local(
        DB_PATH, embeddings, allow_dangerous_deserialization=True
    )

# 2Ô∏è‚É£ Busca documentos similares
def query_documents(query: str, k: int = 2):
    db = load_vectorstore()
    return db.similarity_search(query, k=k)

# 3Ô∏è‚É£ Inicializa o modelo local
llm = Ollama(
    model="phi3",
    temperature=0
)

# 4Ô∏è‚É£ Loop do chat
if __name__ == "__main__":
    print("ü§ñ Chat RAG iniciado (digite 'sair' para encerrar)\n")

    while True:
        # Pergunta do usu√°rio
        user_query = input("Voc√™: ")
        if user_query.lower() in ["sair", "exit", "quit"]:
            break

        # Busca no PDF
        docs = query_documents(user_query)

        # Cria o contexto (sem imprimir)
        contexto = "\n\n".join([doc.page_content for doc in docs])

        # üî• PROMPT AQUI
        prompt = f"""
Voc√™ √© um concierge de hotel.
Responda de forma clara, educada e objetiva.
Responda SOMENTE o que foi perguntado.
N√£o inclua informa√ß√µes extras.

Pergunta:
{user_query}

Contexto:
{contexto}

Resposta:
"""
        print("ü§ñ Pensando...")
        resposta = llm.invoke(prompt)

        print("\nü§ñ Resposta:")
        print(resposta)

